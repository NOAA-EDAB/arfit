<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Background • arfit</title>
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha256-bZLfwXAP04zRMK2BjiO8iu9pf4FbLqX6zitd+tIvLhE=" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><meta property="og:title" content="Background">
<meta property="og:description" content="arfit">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">
    

    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">arfit</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="">0.0.0.9000</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../articles/arfit.html">Get started</a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/background_work.html">Background</a>
    </li>
  </ul>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/andybeet/arfit/" class="external-link">
    <span class="fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><script src="background_work_files/accessible-code-block-0.0.1/empty-anchor.js"></script><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>Background</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/andybeet/arfit/blob/HEAD/vignettes/background_work.Rmd" class="external-link"><code>vignettes/background_work.Rmd</code></a></small>
      <div class="hidden name"><code>background_work.Rmd</code></div>

    </div>

    
    
<p>The model we are interested in is</p>
<p><span class="math display">\[y_t = \beta_0 + \beta_1 t + u_t\]</span> where <span class="math inline">\(t\)</span> refers to time and <span class="math inline">\(u_t\)</span> is either</p>
<ol style="list-style-type: decimal">
<li>
<span class="math inline">\(u_t = \phi u_{t-1} + e_t\)</span>                    an AR(1) process</li>
<li>
<span class="math inline">\(u_t = \phi_1 u_{t-1} + \phi_2 u_{t-2} + e_t\)</span>         an AR(2) process</li>
</ol>
<p>with <span class="math inline">\(e_t \sim N(0,\sigma^2)\)</span></p>
<div class="section level3">
<h3 id="likelihood-for-arp-errors">Likelihood for AR(p) errors<a class="anchor" aria-label="anchor" href="#likelihood-for-arp-errors"></a>
</h3>
<p>From <span class="citation">Hamilton (<a href="#ref-hamilton_time_1994" role="doc-biblioref">1994</a>)</span></p>
<p><span class="math display">\[\begin{align}
logL\left( \underline{\beta}, \underline{\phi},\sigma; \underline{y} \right ) &amp;= -\frac{n}{2}log(2\pi) -\frac{n}{2}log(\sigma^2) +\frac{1}{2}log \left|V_p^{-1} \right|
\\
&amp;-\frac{1 }{2 \sigma^2} (\underline{y_p}-\underline{\mu_p})^T V_p^{-1}(\underline{y_p}-\underline{\mu_p}) \\

&amp;- \frac{1}{2\sigma^2}\sum^n_{t=p+1} (y_t - c - \phi_1y_{t-1} - ... - \phi_p y_{t-p})^2 \\

\end{align}\]</span></p>
<p>where</p>
<p><span class="math inline">\(\left|V_p^{-1} \right|\)</span> is determinant of inverted matrix <span class="math inline">\(V_p\)</span>,</p>
<p><span class="math inline">\(\sigma^2V_p\)</span> = variance-covariance matrix of order p,</p>
<p><span class="math inline">\(\underline{\mu_p} = X_p\underline{\beta}\)</span>, and</p>
<p><span class="math inline">\(X_p\)</span> is the <span class="math inline">\(p_{th}\)</span> row of the design matrix corresponding to time t = p</p>
<p><span class="math inline">\(c\)</span> = function of fitted terms <span class="math inline">\(X_t\underline{\beta}\)</span></p>
</div>
<div class="section level3">
<h3 id="likelihood-for-ar1-errors">Likelihood for AR(1) errors<a class="anchor" aria-label="anchor" href="#likelihood-for-ar1-errors"></a>
</h3>
<p>Setting p = 1,</p>
<p><span class="math inline">\(\sigma^2V_1 = \frac{\sigma^2}{1-\phi_1^2}\)</span> = variance of the process, y,</p>
<p><span class="math inline">\(\left|V_1^{-1}\right| = 1-\phi_1^2\)</span></p>
<p><span class="math inline">\(\mu_1 = X_1\underline{\beta} = \beta_0 + \beta_1\)</span></p>
<p><span class="math inline">\(c = X_t\underline{\beta} - \phi_1 X_{t-1}\underline{\beta}\)</span></p>
<p><span class="math inline">\(y_1\)</span> is the first observation</p>
<p>The log likelihood,</p>
<p><span class="math display">\[\begin{align}

logL\left( \underline{\beta}, \phi_1, \sigma; \underline{y} \right ) &amp;= -\frac{n}{2}log(2\pi) -\frac{n}{2}log(\sigma^2) +\frac{1}{2}log(1-\phi_1^2) 
\\
&amp;-\frac{1 }{2 \sigma^2} ({y_1}-X_1\underline{\beta})^2 (1-\phi_1^2) \\
&amp;- \frac{1}{2\sigma^2}\sum^n_{t=2} (y_t - X_t\underline{\beta} - \phi_1 (y_{t-1} -X_{t-1}\underline{\beta}))^2 \\

\end{align}\]</span></p>
<p>Differentiating <span class="math inline">\(logL\left( \underline{\beta}, \phi_1, \sigma; \underline{y} \right )\)</span> with respect to <span class="math inline">\(\sigma\)</span> and equating to zero yields the maximum likelihood estimator,</p>
<p><span class="math display">\[\begin{align}
\hat{\sigma}^2 = \frac{1}{n}\left[ (y_1-X_1\underline{\beta})^2(1-\phi_1^2) + \sum^n_{t=2} (y_t - X_t\underline{\beta} - \phi_1 (y_{t-1} -X_{t-1}\underline{\beta}))^2 \right]

\end{align}\]</span></p>
<p>Substituting <span class="math inline">\(\hat{\sigma}^2\)</span> back into the log likelihood yields (<span class="citation">Beach and MacKinnon (<a href="#ref-beach_maximum_1978" role="doc-biblioref">1978</a><a href="#ref-beach_maximum_1978" role="doc-biblioref">a</a>)</span>)</p>
<p><span class="math display">\[\begin{align}
logL\left( \underline{\beta}, \phi_1; \underline{y} \right ) &amp;= const. +\frac{1}{2}log(1-\phi^2) 
\\
&amp;-\frac{n}{2}log\left( (y_1-X_1\underline{\beta})^2(1-\phi_1^2) + \sum^n_{t=2} (y_t - X_t\underline{\beta} - \phi_1 (y_{t-1} -X_{t-1}\underline{\beta}))^2 \right) \\
\end{align}\]</span></p>
<p>Two additional terms exist in the likelihood that do not appear in the conditional likelihood used in GLS procedures. The term <span class="math inline">\((y_1-X_1\underline{\beta})^2(1-\phi_1^2)\)</span> ensures that the initial value has an effect on the estimates and <span class="math inline">\(\frac{1}{2}log(1-\phi^2)\)</span> constrains the stationary condition to hold.</p>
<div class="section level4">
<h4 id="maximization-of-the-likelihood">Maximization of the likelihood<a class="anchor" aria-label="anchor" href="#maximization-of-the-likelihood"></a>
</h4>
<p>Maximization of the likelihood requires iterative or numerical procedures.</p>
<p>The estimate of <span class="math inline">\(\underline{\beta}\)</span> which maximizes the log-likelihood conditional on <span class="math inline">\(\phi\)</span> is</p>
<p><span class="math display">\[\begin{align}
\underline{\hat{\beta}} = (X^{*T}X^*)^{-1}X^{*T}y^*
\end{align}\]</span></p>
<p>where <span class="math inline">\(X^*=QX\)</span> and <span class="math inline">\(y^* = Qy\)</span> where <span class="math inline">\(Q\)</span> is the Prais and Winsten transformation</p>
<p><span class="math display">\[ 
\begin{matrix} Q =
\end{matrix}
\begin{bmatrix}
(1-\phi_1^2)^{1/2} &amp; 0 &amp; ... \\
-\phi_1 &amp; 1 &amp; 0 &amp; ... \\
&amp; &amp; ... &amp; 0 &amp; -\phi_1 &amp; 1\\
\end{bmatrix}
\]</span> So a procedure which searches for <span class="math inline">\(\phi_1\)</span> is all that is required to find maximum likelihood estimates of <span class="math inline">\(\underline\beta\)</span>, <span class="math inline">\(\sigma\)</span>, and <span class="math inline">\(\phi_1\)</span></p>
</div>
</div>
<div class="section level3">
<h3 id="likelihood-for-ar2-errors">Likelihood for AR(2) errors<a class="anchor" aria-label="anchor" href="#likelihood-for-ar2-errors"></a>
</h3>
<p>Following the same method as for AR(1) errors.</p>
<p>Set p = 2,</p>
<p>\begin{matrix} V_2^{-1} = \end{matrix} \begin{bmatrix} (1-_2^2) &amp; -(_1 + _1_2) \ -(_1 + _1_2) &amp; (1-_2^2) \ \end{bmatrix}</p>
<p><span class="math inline">\(\left|V_2^{-1}\right| = (1+\phi_2^2)\left[(1-\phi_2)^2 -\phi_1^2 \right]\)</span></p>
<p><span class="math inline">\(\underline{\mu_2} = (\mu_1, \mu_2) = (X_1\underline{\beta},X_2\underline{\beta})\)</span> is the vector of means for t = 1,2 and <span class="math inline">\(\underline{y_2} = (y_1,y_2)\)</span> is the corresponding vector of observations</p>
<p><span class="math display">\[
\begin{bmatrix} 
\mu_1 \\
\mu_2 \\
\end{bmatrix} =
\begin{bmatrix} 
1 &amp; 1 \\
1 &amp; 2 \\
\end{bmatrix}
\begin{bmatrix} 
\beta_0 \\
\beta_1 \\
\end{bmatrix}
\]</span></p>
<p><span class="math inline">\(c = X_t\underline{\beta} - \phi_1 X_{t-1}\underline{\beta}- \phi_2 X_{t-2}\underline{\beta}\)</span></p>
<p>The log likelihood,</p>
<p><span class="math display">\[\begin{align}

logL\left( \underline{\beta}, \underline{\phi}, \sigma; \underline{y} \right ) &amp;= -\frac{n}{2}log(2\pi) -\frac{n}{2}log(\sigma^2) +\frac{1}{2}log((1+\phi_2^2)\left[(1-\phi_2)^2 -\phi_1^2 \right]) 
\\
&amp;-\frac{1 }{2 \sigma^2} (\underline{y_2}-\underline{\mu_2})^T V_2^{-1}(\underline{y_2}-\underline{\mu_2}) \\
&amp;- \frac{1}{2\sigma^2}\sum^n_{t=3} (y_t - X_t\underline{\beta} - \phi_1 (y_{t-1} -X_{t-1}\underline{\beta}) - \phi_2 (y_{t-2} -X_{t-2}\underline{\beta}))^2 \\

\end{align}\]</span></p>
<p>Differentiating <span class="math inline">\(logL\left( \underline{\beta}, \underline{\phi}, \sigma; \underline{y} \right )\)</span> with respect to <span class="math inline">\(\sigma\)</span> and equating to zero yields the maximum likelihood estimator,</p>
<p><span class="math display">\[\begin{align}
\hat{\sigma}^2 = \frac{1}{n}\left[ (\underline{y_2}-\underline{\mu_2})^T V_2^{-1}(\underline{y_2}-\underline{\mu_2}) + \sum^n_{t=3} (y_t - X_t\underline{\beta} - \phi_1 (y_{t-1} -X_{t-1}\underline{\beta}) - \phi_2 (y_{t-2} -X_{t-2}\underline{\beta}))^2\right]

\end{align}\]</span></p>
<p>Substituting <span class="math inline">\(\hat{\sigma}^2\)</span> back into the log likelihood yields</p>
<p><span class="math display">\[\begin{align}
logL\left( \underline{\beta}, \underline{\phi}; \underline{y} \right) &amp;= const. +\frac{1}{2}log\left((1+\phi_2^2)\left[(1-\phi_2)^2 -\phi_1^2 \right]\right) 
\\
&amp;-\frac{n}{2}log\left[ (\underline{y_2}-\underline{\mu_2})^T V_2^{-1}(\underline{y_2}-\underline{\mu_2}) + \sum^n_{t=3} (y_t - X_t\underline{\beta} - \phi_1 (y_{t-1} -X_{t-1}\underline{\beta}) - \phi_2 (y_{t-2} -X_{t-2}\underline{\beta}))^2\right]\\
\end{align}\]</span></p>
<p>Simplifying further,</p>
<p><span class="math display">\[\begin{align}
\frac{1}{2}log\left((1+\phi_2^2)\left[(1-\phi_2)^2 -\phi_1^2 \right]\right) = log(1+\phi_2) + \frac{1}{2}log(1-\phi_1-\phi_2)+\frac{1}{2}log(1+\phi_1-\phi_2)
\end{align}\]</span></p>
<p>and</p>
<p><span class="math display">\[\begin{align}
(\underline{y_2}-\underline{\mu_2})^T V_2^{-1}(\underline{y_2}-\underline{\mu_2}) = &amp;
\begin{bmatrix}
(y_1 - \mu_1) &amp; (y_2 -\mu_2) \\
\end{bmatrix} 

\begin{bmatrix}
(1-\phi_2^2) &amp; -(\phi_1 + \phi_1\phi_2) \\
-(\phi_1 + \phi_1\phi_2) &amp; (1-\phi_2^2)  \\
\end{bmatrix}

\begin{bmatrix}
(y_1 - \mu_1) \\
(y_2 - \mu_2) \\
\end{bmatrix}
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
= &amp; (y_1 - \mu_1)^2(1-\phi_2^2)-2(y_1 - \mu_1)(y_2 - \mu_2)(\phi_1 + \phi_1\phi_2)+(y_2 - \mu_2)^2(1-\phi_2^2)\\
= &amp; (y_1 - \mu_1)^2(1-\phi_2^2)-2(y_1 - \mu_1)(y_2 - \mu_2)\phi_1(1 + \phi_2)+(y_2 - \mu_2)^2(1-\phi_2^2) \\
= &amp; (y_1 - X_1\underline{\beta})^2(1-\phi_2^2)-2(y_1-X_1\underline{\beta})(y_2-X_2\underline{\beta})\phi_1(1 + \phi_2)+(y_2 -X_2\underline{\beta})^2(1-\phi_2^2)
\end{align}\]</span></p>
<p>The likelihood now has the form of that found in <span class="citation">Beach and MacKinnon (<a href="#ref-beach_full_1978" role="doc-biblioref">1978</a><a href="#ref-beach_full_1978" role="doc-biblioref">b</a>)</span></p>
<p><span class="math display">\[\begin{align}
logL\left( \underline{\beta}, \underline{\phi}; \underline{y} \right) &amp;= const. +log(1+\phi_2) + \frac{1}{2}log(1-\phi_1-\phi_2)+\frac{1}{2}log(1+\phi_1-\phi_2)
\\
&amp;-\frac{n}{2}log\left[ (y_1 - X_1\underline{\beta})^2(1-\phi_2^2)-2(y_1-X_1\underline{\beta})(y_2-X_2\underline{\beta})\phi_1(1 + \phi_2)+(y_2 -X_2\underline{\beta})^2(1-\phi_2^2) \\
+ \sum^n_{t=3} (y_t - X_t\underline{\beta} - \phi_1 (y_{t-1} -X_{t-1}\underline{\beta}) - \phi_2 (y_{t-2} -X_{t-2}\underline{\beta}))^2\right]\\
\end{align}\]</span></p>
<div class="section level4">
<h4 id="maximization-of-the-likelihood-1">Maximization of the likelihood<a class="anchor" aria-label="anchor" href="#maximization-of-the-likelihood-1"></a>
</h4>
<p>As in the case for AR(1) model maximizing the likelihood requires numerical methods.</p>
<p>The estimate of <span class="math inline">\(\underline{\beta}\)</span> which maximizes the log-likelihood conditional on <span class="math inline">\(\phi\)</span> (<span class="citation">Beach and MacKinnon (<a href="#ref-beach_full_1978" role="doc-biblioref">1978</a><a href="#ref-beach_full_1978" role="doc-biblioref">b</a>)</span>) is</p>
<p><span class="math display">\[\begin{align}
\underline{\hat{\beta}} = (X^{*T}X^*)^{-1}X^{*T}y^*
\end{align}\]</span></p>
<p>where <span class="math inline">\(X^*=QX\)</span> and <span class="math inline">\(y^* = Qy\)</span> where <span class="math inline">\(Q\)</span> is the Prais and Winsten transformation</p>
<p><span class="math display">\[ 
\begin{matrix} Q =
\end{matrix}
\begin{bmatrix}
\left[(1-\phi_2^2)-\phi_1^2(1+\phi_2)/(1-\phi_2)\right]^{1/2} &amp; 0 &amp; ... \\
\left[\phi_1^2(1+\phi_2)/(1-\phi_2)\right]^{1/2} &amp; (1-\phi_2^2)^{1/2} &amp; 0 &amp; ... \\
-\phi_2 &amp; -\phi_1 &amp; 1 &amp; 0 &amp; ...\\
0 &amp; -\phi_2 &amp; -\phi_1 &amp; 1 &amp; 0 &amp; ...\\
... \\
0 &amp; ... &amp;0  &amp; -\phi_2 &amp; -\phi_1 &amp; 1\\
\end{bmatrix}
\]</span></p>
<p>So a procedure which searches for <span class="math inline">\(\phi_1\)</span> &amp; <span class="math inline">\(\phi_2\)</span> is all that is required to find maximum likelihood estimates of <span class="math inline">\(\underline\beta\)</span>, <span class="math inline">\(\sigma\)</span>, and <span class="math inline">\(\underline{\phi}\)</span></p>
</div>
</div>
<div class="section level3">
<h3 id="likelihood-from-first-principles">Likelihood from first principles<a class="anchor" aria-label="anchor" href="#likelihood-from-first-principles"></a>
</h3>
<div class="section level4">
<h4 id="ar1">AR(1)<a class="anchor" aria-label="anchor" href="#ar1"></a>
</h4>
<p><span class="math display">\[L(\underline{\theta};\underline{y})=\prod_{t=2}^n p(Y_t = y_t │ Y_{t-1}=y_{t-1}) × p(Y_1=y_1)\]</span></p>
<p>where <span class="math inline">\(\underline{\theta} = (\beta_0,\beta_1,\phi,\sigma^2)\)</span>, <span class="math inline">\(p(Y_t = y_t │ Y_{t-1}=y_{t-1})\)</span> is the conditional distribution of <span class="math inline">\(y_t\)</span> given <span class="math inline">\(y_{t-1}\)</span> and <span class="math inline">\(p(Y_1=y_1)\)</span> is the distribution of the first point.</p>
<p>This is the Exact likelihood for an AR(1) process. The conditional likelihood multiplied by the marginal likelihood of the first point.</p>
</div>
<div class="section level4">
<h4 id="density-of-py_1y_1">Density of <span class="math inline">\(p(Y_1=y_1)\)</span><a class="anchor" aria-label="anchor" href="#density-of-py_1y_1"></a>
</h4>
<p><span class="math inline">\(p(Y_1=y_1)\)</span> is normally distributed with mean = <span class="math inline">\(X_1\underline{\beta}\)</span> which equates to <span class="math inline">\(\beta_0 + \beta_1\)</span> with a variance = <span class="math inline">\(\frac{\sigma^2}{1-\phi^2}\)</span> and has density:</p>
<p><span class="math display">\[p(Y_1=y_1) = \frac{1}{\sqrt{2\pi}\sqrt{\sigma^2/(1-\phi^2)}}exp \left( -\frac{1}{2}\left( \frac{ y_1 - \beta_0-\beta_1 }{\sigma/\sqrt(1-\phi^2)}\right)^2\right)\]</span></p>
</div>
<div class="section level4">
<h4 id="density-of-py_t-y_t-y_t-1y_t-1">Density of <span class="math inline">\(p(Y_t = y_t │ Y_{t-1}=y_{t-1})\)</span><a class="anchor" aria-label="anchor" href="#density-of-py_t-y_t-y_t-1y_t-1"></a>
</h4>
<p>The conditional distribution of <span class="math inline">\(p(Y_t = y_t │ Y_{t-1}=y_{t-1})\)</span> is also normally distributed but we get to it in a round about way:</p>
<p>Recall <span class="math inline">\(y_t = \beta_0 + \beta_1 t + u_t\)</span> then <span class="math inline">\(\phi y_{t-1} = \phi \beta_0 + \phi \beta_1 (t-1) + \phi u_{t-1}\)</span></p>
<p>So</p>
<p><span class="math display">\[y_t - \phi y_{t-1} = \beta_0 + \beta_1 t + u_t - \phi \beta_0 - \phi\beta_1(t-1) -\phi u_{t-1}\]</span></p>
<p><span class="math display">\[y_t   = \phi y_{t-1} + \beta_0(1-\phi) + \beta_1 (t - \phi t+\phi) + e_t\]</span></p>
<p>Rearrange to obtain</p>
<p><span class="math display">\[y_t   - \phi y_{t-1} - \beta_0(1-\phi) - \beta_1 (t - \phi t+\phi) = e_t\]</span> which has a normal distribution with mean = 0 and variance = <span class="math inline">\(\sigma^2\)</span></p>
<p>This results in</p>
<p><span class="math display">\[p(Y_t = y_t │ Y_{t-1}=y_{t-1}) = \prod_{t=2}^n \frac{1}{\sigma \sqrt{2\pi}}exp \left( -\frac{1}{2}\left( \frac{ y_t   - \phi y_{t-1} - \beta_0(1-\phi) - \beta_1 (t - \phi t+\phi)  }{\sigma}\right)^2\right)\]</span></p>
</div>
<div class="section level4">
<h4 id="exact-likelihood-for-ar1">Exact likelihood for AR1<a class="anchor" aria-label="anchor" href="#exact-likelihood-for-ar1"></a>
</h4>
<p>The likelihood is therefore:</p>
<p><span class="math display">\[L(\underline{\theta};\underline{y}) = \prod_{t=2}^n \frac{1}{\sigma \sqrt{2\pi}}exp \left( -\frac{1}{2}\left( \frac{ y_t   - \phi y_{t-1} - \beta_0(1-\phi) - \beta_1 (t - \phi t+\phi)  }{\sigma}\right)^2\right) \times \frac{1}{\sqrt{2\pi}\sqrt{\sigma^2/(1-\phi^2)}}exp \left( -\frac{1}{2}\left( \frac{ y_1 - \beta_0 -\beta_1 }{\sigma/\sqrt(1-\phi^2)}\right)^2\right)\]</span></p>
<p>Taking the logs and simplifying results in:</p>
<p><span class="math display">\[ logL(\underline{\theta};\underline{y}) = -nlog\sigma -\frac{n}{2}log(2\pi)-\frac{1}{2}log(1-\phi^2) -\frac{1}{2\sigma^2} \left( (y_1-\beta_0 -\beta_1)^2(1-\phi^2) + \Sigma_{t=2}^n (y_t-\phi y_{t-1}-\beta_0(1-\phi)-\beta_1(t-\phi t + \phi))^2 \right)\]</span> Note that</p>
<p><span class="math display">\[\begin{align}

&amp;\beta_0(1-\phi) + \beta_1 (t - \phi t+\phi) \\
&amp;= \beta_0 -\beta_0\phi + \beta_1t - \beta_1\phi t + \beta_1\phi \\
&amp;= \beta_0 + \beta_1t - \phi (\beta_0 +  \beta_1(t -1)) \\
&amp;= X_t\underline{\beta} - \phi X_{t-1}\underline{\beta}

\end{align}\]</span></p>
<p>Using the notation of <span class="citation">Beach and MacKinnon (<a href="#ref-beach_maximum_1978" role="doc-biblioref">1978</a><a href="#ref-beach_maximum_1978" role="doc-biblioref">a</a>)</span> we can simplify the log likelihood, <span class="math display">\[\begin{align}
 logL(\underline{\theta};\underline{y}) &amp;= -nlog\sigma -\frac{n}{2}log(2\pi)-\frac{1}{2}log(1-\phi^2) \\ &amp;-\frac{1}{2\sigma^2} \left( (y_1-X_1\underline{\beta})^2(1-\phi^2) + \Sigma_{t=2}^n (y_t- X_t\underline{\beta} - \phi( y_{t-1}- X_{t-1}\underline{\beta})^2 \right)\\
\end{align}\]</span></p>
<p>and substituting <span class="math inline">\(\hat\sigma^2\)</span> we get</p>
<p><span class="math display">\[\begin{align}
logL\left( \underline{\beta}, \phi; \underline{y} \right ) &amp;= const. +\frac{1}{2}log(1-\phi^2) 
\\
&amp;-\frac{n}{2}log\left( (y_1-X_1\underline{\beta})^2(1-\phi^2) + \sum^n_{t=2} (y_t - X_t\underline{\beta} - \phi (y_{t-1} -X_{t-1}\underline{\beta}))^2 \right) \\
\end{align}\]</span></p>
</div>
<div class="section level4">
<h4 id="ar2">AR(2)<a class="anchor" aria-label="anchor" href="#ar2"></a>
</h4>
<p><span class="math display">\[L(\underline{\theta};\underline{y})=\prod_{t=2}^n p(Y_t=y_t│Y_{t-1}=y_{t-1},Y_{t-2}=y_{t-2}) \times p(Y_2=y_2 | Y_1=y_1) \times p(Y_1=y_1)\]</span></p>
<p>Following the same reasoning we can obtain the densities of each of the three components of the likelihood. The exact likelihood is the product of the conditional likelihood, the conditional distribution of <span class="math inline">\(y_2 | y_1\)</span> and the marginal distribution of <span class="math inline">\(y_1\)</span>.</p>
</div>
<div class="section level4">
<h4 id="density-of-py_1y_1-1">Density of <span class="math inline">\(p(Y_1=y_1)\)</span><a class="anchor" aria-label="anchor" href="#density-of-py_1y_1-1"></a>
</h4>
<p><span class="math inline">\(p(Y_1=y_1)\)</span> is normally distributed with mean = <span class="math inline">\(X_1\underline{\beta}\)</span> which equates to <span class="math inline">\(\beta_0 + \beta_1\)</span> with a variance = <span class="math inline">\(\frac{\sigma^2}{1-\phi_1^2 - \phi_2^2}\)</span></p>
</div>
<div class="section level4">
<h4 id="density-of-py_2y_2y_1y_1">Density of <span class="math inline">\(p(Y_2=y_2│Y_1=y_1)\)</span><a class="anchor" aria-label="anchor" href="#density-of-py_2y_2y_1y_1"></a>
</h4>
<p><span class="math inline">\(p(Y_2=y_2│Y_1=y_1)\)</span> is more complicated. Need to work this out</p>
</div>
<div class="section level4">
<h4 id="density-of-py_t-y_t-y_t-1y_t-1y_t-2y_t-2">Density of <span class="math inline">\(p(Y_t = y_t │ Y_{t-1}=y_{t-1},Y_{t-2}=y_{t-2})\)</span><a class="anchor" aria-label="anchor" href="#density-of-py_t-y_t-y_t-1y_t-1y_t-2y_t-2"></a>
</h4>
<p>Recall <span class="math inline">\(y_t = \beta_0 + \beta_1 t + u_t\)</span> then <span class="math inline">\(\phi_1 y_{t-1} = \phi_1 \beta_0 + \phi_1 \beta_1 (t-1) + \phi_1 u_{t-1}\)</span> and <span class="math inline">\(\phi_2 y_{t-2} = \phi_2 \beta_0 + \phi_2 \beta_1 (t-2) + \phi_2 u_{t-2}\)</span></p>
<p>So</p>
<p><span class="math display">\[y_t - \phi_1 y_{t-1} - \phi_2 y_{t-2} = \beta_0 + \beta_1 t + u_t - \phi_1 \beta_0 - \phi_1\beta_1(t-1) -\phi_1 u_{t-1} - \phi_2 \beta_0 - \phi_2\beta_1(t-2) -\phi_2 u_{t-2}\]</span> which simplifies to</p>
<p><span class="math display">\[y_t - \phi_1 y_{t-1} - \phi_2 y_{t-2} - \beta_0 (1-\phi_1-\phi_2) - \beta_1 (t - \phi_1(t-1)- \phi_2(t-2))  = e_t\]</span> which results in</p>
<p><span class="math display">\[p(Y_t = y_t │ Y_{t-1}=y_{t-1},Y_{t-2}=y_{t-2}) = \prod_{t=3}^n \frac{1}{\sigma \sqrt{2\pi}}exp \left( -\frac{1}{2}\left( \frac{ y_t - \phi_1 y_{t-1} - \phi_2 y_{t-2} - \beta_0 (1-\phi_1-\phi_2) - \beta_1 (t - \phi_1(t-1)- \phi_2(t-2))  }{\sigma}\right)^2\right)\]</span></p>
</div>
</div>
<div class="section level2">
<h2 id="exact-likelihood-for-ar2">Exact likelihood for AR2<a class="anchor" aria-label="anchor" href="#exact-likelihood-for-ar2"></a>
</h2>
<p>The likelihood is therefore:</p>
</div>
<div class="section level2 unnumbered">
<h2 id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<div id="refs" class="references">
<div id="ref-beach_maximum_1978">
<p>Beach, Charles M., and James G. MacKinnon. 1978a. “A Maximum Likelihood Procedure for Regression with Autocorrelated Errors.” <em>Econometrica</em> 46 (1): 51. <a href="https://doi.org/10.2307/1913644" class="external-link">https://doi.org/10.2307/1913644</a>.</p>
</div>
<div id="ref-beach_full_1978">
<p>———. 1978b. “Full Maximum Likelihood Estimation of Second- Order Autoregressive Error Models.” <em>Journal of Econometrics</em> 7 (2): 187–98. <a href="https://doi.org/10.1016/0304-4076(78)90068-4" class="external-link">https://doi.org/10.1016/0304-4076(78)90068-4</a>.</p>
</div>
<div id="ref-hamilton_time_1994">
<p>Hamilton, James D. 1994. <em>Time Series Analysis</em>. Princeton, N.J: Princeton University Press.</p>
</div>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

        <nav id="toc" data-toggle="toc"><h2 data-toc-skip>Contents</h2>
    </nav>
</div>

</div>



      <footer><div class="copyright">
  <p></p>
<p>Developed by Andy Beet.</p>
</div>

<div class="pkgdown">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.2.</p>
</div>

      </footer>
</div>

  


  

  </body>
</html>
